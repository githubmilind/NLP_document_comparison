{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d846ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import brown\n",
    "import math\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e0d923f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all-corpora'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\extended_omw.zip.\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    | Downloading package omw to C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]    |     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all-corpora\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Milind.DESKTOP-\n",
      "[nltk_data]     GBR1BS2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all-corpora')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0632fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to the algorithm. Currently set to values that was reported\n",
    "# in the paper to produce \"best\" results.\n",
    "ALPHA = 0.2\n",
    "BETA = 0.45\n",
    "ETA = 0.4\n",
    "PHI = 0.2\n",
    "DELTA = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b309b1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_freqs = dict()\n",
    "N = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956b19e6",
   "metadata": {},
   "source": [
    "######################## word similarity ##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79d720c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_synset_pair(word_1, word_2):\n",
    "    \"\"\" \n",
    "    Choose the pair with highest path similarity among all pairs. \n",
    "    Mimics pattern-seeking behavior of humans.\n",
    "    \"\"\"\n",
    "    max_sim = -1.0\n",
    "    synsets_1 = wn.synsets(word_1)\n",
    "    synsets_2 = wn.synsets(word_2)\n",
    "    if len(synsets_1) == 0 or len(synsets_2) == 0:\n",
    "        return None, None\n",
    "    else:\n",
    "        max_sim = -1.0\n",
    "        best_pair = None, None\n",
    "        for synset_1 in synsets_1:\n",
    "            for synset_2 in synsets_2:\n",
    "                sim = wn.path_similarity(synset_1, synset_2)\n",
    "                if sim == None:\n",
    "                    sim = 0.\n",
    "                if sim > max_sim:\n",
    "                    max_sim = sim\n",
    "                    best_pair = synset_1, synset_2\n",
    "        return best_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f307e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_dist(synset_1, synset_2):\n",
    "    \"\"\"\n",
    "    Return a measure of the length of the shortest path in the semantic \n",
    "    ontology (Wordnet in our case as well as the paper's) between two \n",
    "    synsets.\n",
    "    \"\"\"\n",
    "    l_dist = sys.maxsize\n",
    "    if synset_1 is None or synset_2 is None: \n",
    "        return 0.0\n",
    "    if synset_1 == synset_2:\n",
    "        # if synset_1 and synset_2 are the same synset return 0\n",
    "        l_dist = 0.0\n",
    "    else:\n",
    "        wset_1 = set([str(x.name()) for x in synset_1.lemmas()])        \n",
    "        wset_2 = set([str(x.name()) for x in synset_2.lemmas()])\n",
    "        if len(wset_1.intersection(wset_2)) > 0:\n",
    "            # if synset_1 != synset_2 but there is word overlap, return 1.0\n",
    "            l_dist = 1.0\n",
    "        else:\n",
    "            # just compute the shortest path between the two\n",
    "            l_dist = synset_1.shortest_path_distance(synset_2)\n",
    "            if l_dist is None:\n",
    "                l_dist = 0.0\n",
    "    # normalize path length to the range [0,1]\n",
    "    return math.exp(-ALPHA * l_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4f486f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchy_dist(synset_1, synset_2):\n",
    "    \"\"\"\n",
    "    Return a measure of depth in the ontology to model the fact that \n",
    "    nodes closer to the root are broader and have less semantic similarity\n",
    "    than nodes further away from the root.\n",
    "    \"\"\"\n",
    "    h_dist = sys.maxsize\n",
    "    if synset_1 is None or synset_2 is None: \n",
    "        return h_dist\n",
    "    if synset_1 == synset_2:\n",
    "        # return the depth of one of synset_1 or synset_2\n",
    "        h_dist = max([x[1] for x in synset_1.hypernym_distances()])\n",
    "    else:\n",
    "        # find the max depth of least common subsumer\n",
    "        hypernyms_1 = {x[0]:x[1] for x in synset_1.hypernym_distances()}\n",
    "        hypernyms_2 = {x[0]:x[1] for x in synset_2.hypernym_distances()}\n",
    "        lcs_candidates = set(hypernyms_1.keys()).intersection(\n",
    "            set(hypernyms_2.keys()))\n",
    "        if len(lcs_candidates) > 0:\n",
    "            lcs_dists = []\n",
    "            for lcs_candidate in lcs_candidates:\n",
    "                lcs_d1 = 0\n",
    "                if lcs_candidate in hypernyms_1 :\n",
    "                    lcs_d1 = hypernyms_1[lcs_candidate]\n",
    "                lcs_d2 = 0\n",
    "                if lcs_candidate in hypernyms_2:\n",
    "                    lcs_d2 = hypernyms_2[lcs_candidate]\n",
    "                lcs_dists.append(max([lcs_d1, lcs_d2]))\n",
    "            h_dist = max(lcs_dists)\n",
    "        else:\n",
    "            h_dist = 0\n",
    "    return ((math.exp(BETA * h_dist) - math.exp(-BETA * h_dist)) / \n",
    "        (math.exp(BETA * h_dist) + math.exp(-BETA * h_dist)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4fccae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity(word_1, word_2):\n",
    "    synset_pair = get_best_synset_pair(word_1, word_2)\n",
    "    return (length_dist(synset_pair[0], synset_pair[1]) * \n",
    "        hierarchy_dist(synset_pair[0], synset_pair[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdab62f2",
   "metadata": {},
   "source": [
    "######################### sentence similarity ##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f27be736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_word(word, word_set):\n",
    "    \"\"\"\n",
    "    Find the word in the joint word set that is most similar to the word\n",
    "    passed in. We use the algorithm above to compute word similarity between\n",
    "    the word and each word in the joint word set, and return the most similar\n",
    "    word and the actual similarity value.\n",
    "    \"\"\"\n",
    "    max_sim = -1.0\n",
    "    sim_word = \"\"\n",
    "    for ref_word in word_set:\n",
    "      sim = word_similarity(word, ref_word)\n",
    "      if sim > max_sim:\n",
    "          max_sim = sim\n",
    "          sim_word = ref_word\n",
    "    return sim_word, max_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22d74bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_content(lookup_word):\n",
    "    \"\"\"\n",
    "    Uses the Brown corpus available in NLTK to calculate a Laplace\n",
    "    smoothed frequency distribution of words, then uses this information\n",
    "    to compute the information content of the lookup_word.\n",
    "    \"\"\"\n",
    "    global N\n",
    "    if N == 0:\n",
    "        # poor man's lazy evaluation\n",
    "        for sent in brown.sents():\n",
    "            for word in sent:\n",
    "                word = word.lower()\n",
    "                if not (word in brown_freqs):\n",
    "                    brown_freqs[word] = 0\n",
    "                brown_freqs[word] = brown_freqs[word] + 1\n",
    "                N = N + 1\n",
    "    lookup_word = lookup_word.lower()\n",
    "    n = 0 if not (lookup_word in brown_freqs) else brown_freqs[lookup_word]\n",
    "    return 1.0 - (math.log(n + 1) / math.log(N + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2341434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_vector(words, joint_words, info_content_norm):\n",
    "    \"\"\"\n",
    "    Computes the semantic vector of a sentence. The sentence is passed in as\n",
    "    a collection of words. The size of the semantic vector is the same as the\n",
    "    size of the joint word set. The elements are 1 if a word in the sentence\n",
    "    already exists in the joint word set, or the similarity of the word to the\n",
    "    most similar word in the joint word set if it doesn't. Both values are \n",
    "    further normalized by the word's (and similar word's) information content\n",
    "    if info_content_norm is True.\n",
    "    \"\"\"\n",
    "    sent_set = set(words)\n",
    "    semvec = np.zeros(len(joint_words))\n",
    "    i = 0\n",
    "    for joint_word in joint_words:\n",
    "        if joint_word in sent_set:\n",
    "            # if word in union exists in the sentence, s(i) = 1 (unnormalized)\n",
    "            semvec[i] = 1.0\n",
    "            if info_content_norm:\n",
    "                semvec[i] = semvec[i] * math.pow(info_content(joint_word), 2)\n",
    "        else:\n",
    "            # find the most similar word in the joint set and set the sim value\n",
    "            sim_word, max_sim = most_similar_word(joint_word, sent_set)\n",
    "            semvec[i] = PHI if max_sim > PHI else 0.0\n",
    "            if info_content_norm:\n",
    "                semvec[i] = semvec[i] * info_content(joint_word) * info_content(sim_word)\n",
    "        i = i + 1\n",
    "    return semvec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d18bf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_similarity(sentence_1, sentence_2, info_content_norm):\n",
    "    \"\"\"\n",
    "    Computes the semantic similarity between two sentences as the cosine\n",
    "    similarity between the semantic vectors computed for each sentence.\n",
    "    \"\"\"\n",
    "    words_1 = nltk.word_tokenize(sentence_1)\n",
    "    words_2 = nltk.word_tokenize(sentence_2)\n",
    "    joint_words = set(words_1).union(set(words_2))\n",
    "    vec_1 = semantic_vector(words_1, joint_words, info_content_norm)\n",
    "    vec_2 = semantic_vector(words_2, joint_words, info_content_norm)\n",
    "    return np.dot(vec_1, vec_2.T) / (np.linalg.norm(vec_1) * np.linalg.norm(vec_2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac6e6a7",
   "metadata": {},
   "source": [
    "######################### word order similarity ##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35f2a05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_order_vector(words, joint_words, windex):\n",
    "    \"\"\"\n",
    "    Computes the word order vector for a sentence. The sentence is passed\n",
    "    in as a collection of words. The size of the word order vector is the\n",
    "    same as the size of the joint word set. The elements of the word order\n",
    "    vector are the position mapping (from the windex dictionary) of the \n",
    "    word in the joint set if the word exists in the sentence. If the word\n",
    "    does not exist in the sentence, then the value of the element is the \n",
    "    position of the most similar word in the sentence as long as the similarity\n",
    "    is above the threshold ETA.\n",
    "    \"\"\"\n",
    "    wovec = np.zeros(len(joint_words))\n",
    "    i = 0\n",
    "    wordset = set(words)\n",
    "    for joint_word in joint_words:\n",
    "        if joint_word in wordset:\n",
    "            # word in joint_words found in sentence, just populate the index\n",
    "            wovec[i] = windex[joint_word]\n",
    "        else:\n",
    "            # word not in joint_words, find most similar word and populate\n",
    "            # word_vector with the thresholded similarity\n",
    "            sim_word, max_sim = most_similar_word(joint_word, wordset)\n",
    "            if max_sim > ETA:\n",
    "                wovec[i] = windex[sim_word]\n",
    "            else:\n",
    "                wovec[i] = 0\n",
    "        i = i + 1\n",
    "    return wovec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c5fd4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_order_similarity(sentence_1, sentence_2):\n",
    "    \"\"\"\n",
    "    Computes the word-order similarity between two sentences as the normalized\n",
    "    difference of word order between the two sentences.\n",
    "    \"\"\"\n",
    "    words_1 = nltk.word_tokenize(sentence_1)\n",
    "    words_2 = nltk.word_tokenize(sentence_2)\n",
    "    joint_words = list(set(words_1).union(set(words_2)))\n",
    "    windex = {x[1]: x[0] for x in enumerate(joint_words)}\n",
    "    r1 = word_order_vector(words_1, joint_words, windex)\n",
    "    r2 = word_order_vector(words_2, joint_words, windex)\n",
    "    return 1.0 - (np.linalg.norm(r1 - r2) / np.linalg.norm(r1 + r2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ded8f33",
   "metadata": {},
   "source": [
    "######################### overall similarity ##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45ed247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(sentence_1, sentence_2, info_content_norm):\n",
    "    \"\"\"\n",
    "    Calculate the semantic similarity between two sentences. The last \n",
    "    parameter is True or False depending on whether information content\n",
    "    normalization is desired or not.\n",
    "    \"\"\"\n",
    "    return DELTA * semantic_similarity(sentence_1, sentence_2, info_content_norm) + \\\n",
    "        (1.0 - DELTA) * word_order_similarity(sentence_1, sentence_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ba00b1",
   "metadata": {},
   "source": [
    "######################### main / test ##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3057031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      asylum\t       fruit\t0.30\n",
      "   autograph\t       shore\t0.00\n",
      "   autograph\t   signature\t0.82\n",
      "  automobile\t         car\t1.00\n",
      "        bird\t    woodland\t0.20\n",
      "         boy\t     rooster\t0.11\n",
      "         boy\t         lad\t0.82\n",
      "         boy\t        sage\t0.37\n",
      "    cemetery\t   graveyard\t1.00\n",
      "       coast\t      forest\t0.36\n",
      "       coast\t       shore\t0.80\n",
      "        cock\t     rooster\t1.00\n",
      "        cord\t       smile\t0.13\n",
      "        cord\t      string\t0.82\n",
      "     cushion\t      pillow\t0.82\n",
      "      forest\t   graveyard\t0.20\n",
      "      forest\t    woodland\t0.98\n",
      "     furnace\t       stove\t0.17\n",
      "       glass\t     tumbler\t0.82\n",
      "        grin\t       smile\t0.99\n",
      "         gem\t       jewel\t1.00\n",
      "        hill\t    woodland\t0.36\n",
      "        hill\t       mound\t0.99\n",
      "   implement\t        tool\t0.82\n",
      "     journey\t      voyage\t0.82\n",
      "    magician\t      oracle\t0.30\n",
      "    magician\t      wizard\t1.00\n",
      "      midday\t        noon\t1.00\n",
      "      oracle\t        sage\t0.37\n",
      "        serf\t       slave\t0.55\n",
      "    interest\t         fee\t0.82\n",
      "     florida\t          fl\t1.00\n"
     ]
    }
   ],
   "source": [
    "# the results of the algorithm are largely dependent on the results of \n",
    "# the word similarities, so we should test this first...\n",
    "word_pairs = [\n",
    "  [\"asylum\", \"fruit\"],\n",
    "  [\"autograph\", \"shore\"],\n",
    "  [\"autograph\", \"signature\"],\n",
    "  [\"automobile\", \"car\"],\n",
    "  [\"bird\", \"woodland\"],\n",
    "  [\"boy\", \"rooster\"],\n",
    "  [\"boy\", \"lad\"],\n",
    "  [\"boy\", \"sage\"],\n",
    "  [\"cemetery\", \"graveyard\"],\n",
    "  [\"coast\", \"forest\"],\n",
    "  [\"coast\", \"shore\"],\n",
    "  [\"cock\", \"rooster\"],\n",
    "  [\"cord\", \"smile\"],\n",
    "  [\"cord\", \"string\"],\n",
    "  [\"cushion\", \"pillow\"],\n",
    "  [\"forest\", \"graveyard\"],\n",
    "  [\"forest\", \"woodland\"],\n",
    "  [\"furnace\", \"stove\"],\n",
    "  [\"glass\", \"tumbler\"],\n",
    "  [\"grin\", \"smile\"],\n",
    "  [\"gem\", \"jewel\"],\n",
    "  [\"hill\", \"woodland\"],\n",
    "  [\"hill\", \"mound\"],\n",
    "  [\"implement\", \"tool\"],\n",
    "  [\"journey\", \"voyage\"],\n",
    "  [\"magician\", \"oracle\"],\n",
    "  [\"magician\", \"wizard\"],\n",
    "  [\"midday\", \"noon\"],\n",
    "  [\"oracle\", \"sage\"],\n",
    "  [\"serf\", \"slave\"],\n",
    "  [\"interest\", \"fee\"],\n",
    "  [\"florida\", \"fl\"]\n",
    "]\n",
    "    \n",
    "for word_pair in word_pairs:\n",
    "    print (\"%12s\\t%12s\\t%.2f\" % (word_pair[0], word_pair[1], word_similarity(word_pair[0], word_pair[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7a24af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         I like that bachelor.\t    I like that unmarried man.\t0.804\n",
      "            John is very nice.\t            Is John very nice?\t0.580\n",
      "          Red alcoholic drink.\t             A bottle of wine.\t0.430\n",
      "          Red alcoholic drink.\t           Fresh orange juice.\t0.451\n",
      "          Red alcoholic drink.\t        An English dictionary.\t0.284\n",
      "          Red alcoholic drink.\t            Fresh apple juice.\t0.395\n",
      "             A glass of cider.\t    A full cup of apple juice.\t0.662\n",
      "                  It is a dog.\t        That must be your dog.\t0.481\n",
      "                  It is a dog.\t                  It is a log.\t0.860\n",
      "                  It is a dog.\t                  It is a pig.\t0.878\n",
      "             Dogs are animals.\t         They are common pets.\t0.570\n",
      " Canis familiaris are animals.\t         Dogs are common pets.\t0.501\n",
      "                 I have a pen.\t            Where do you live?\t0.101\n",
      "                 I have a pen.\t                 Where is ink?\t0.139\n",
      "              I have a hammer.\t              Take some nails.\t0.456\n",
      "              I have a hammer.\t             Take some apples.\t0.377\n"
     ]
    }
   ],
   "source": [
    "sentence_pairs = [\n",
    "    [\"I like that bachelor.\", \"I like that unmarried man.\"],\n",
    "    [\"John is very nice.\", \"Is John very nice?\"],\n",
    "    [\"Red alcoholic drink.\", \"A bottle of wine.\"],\n",
    "    [\"Red alcoholic drink.\", \"Fresh orange juice.\"],\n",
    "    [\"Red alcoholic drink.\", \"An English dictionary.\"],\n",
    "    [\"Red alcoholic drink.\", \"Fresh apple juice.\"],\n",
    "    [\"A glass of cider.\", \"A full cup of apple juice.\"],\n",
    "    [\"It is a dog.\", \"That must be your dog.\"],\n",
    "    [\"It is a dog.\", \"It is a log.\"],\n",
    "    [\"It is a dog.\", \"It is a pig.\"],\n",
    "    [\"Dogs are animals.\", \"They are common pets.\"],\n",
    "    [\"Canis familiaris are animals.\", \"Dogs are common pets.\"],\n",
    "    [\"I have a pen.\", \"Where do you live?\"],\n",
    "    [\"I have a pen.\", \"Where is ink?\"],\n",
    "    [\"I have a hammer.\", \"Take some nails.\"],\n",
    "    [\"I have a hammer.\", \"Take some apples.\"]\n",
    "]\n",
    "for sent_pair in sentence_pairs:\n",
    "    print (\"%30s\\t%30s\\t%.3f\" % (sent_pair[0], sent_pair[1],\n",
    "        similarity(sent_pair[0], sent_pair[1], False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc23f81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you can enjoy a lower interest rate on new eligible charges added to your Pay Over Time feature for limited time.\n",
      "Query: eligible rate will be applied to purchase when you enrolled to pay over time select.\n",
      "Similarity: 0.506\n",
      "\n",
      "you have a pay over time fature on your account that gives you the option to pay certain charges over time with interest\n",
      "Query: eligible rate will be applied to purchase when you enrolled to pay over time select.\n",
      "Similarity: 0.460\n",
      "\n",
      "if you are enrolled in pay over time select, this promotional rate will also apply to eligibile purchase added at your request. \n",
      "Query: eligible rate will be applied to purchase when you enrolled to pay over time select.\n",
      "Similarity: 0.625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_pairs = [\n",
    "    [\"you can enjoy a lower interest rate on new eligible charges added to your Pay Over Time feature for limited time.\"],\n",
    "    [\"you have a pay over time fature on your account that gives you the option to pay certain charges over time with interest\"],\n",
    "    [\"if you are enrolled in pay over time select, this promotional rate will also apply to eligibile purchase added at your request. \"],\n",
    "]\n",
    "\n",
    "query_sentence = \"eligible rate will be applied to purchase when you enrolled to pay over time select.\"\n",
    "\n",
    "for sent_pair in sentence_pairs:\n",
    "    print (\"%s\\nQuery: %s\\nSimilarity: %.3f\\n\" % (sent_pair[0], query_sentence, \n",
    "        similarity(sent_pair[0], query_sentence, False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df566fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
