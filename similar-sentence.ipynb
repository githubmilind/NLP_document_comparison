{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5412d61f",
      "metadata": {
        "id": "5412d61f"
      },
      "source": [
        "https://dev.to/thepylot/compare-documents-similarity-using-python-nlp-4odp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "from google colab"
      ],
      "metadata": {
        "id": "isOEyCMztwGr"
      },
      "id": "isOEyCMztwGr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9155249",
      "metadata": {
        "id": "c9155249"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78622750",
      "metadata": {
        "id": "78622750",
        "outputId": "3d233df3-1844-41cd-8604-d726741be2d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Mars', 'is', 'approximately', 'half', 'the', 'diameter', 'of', 'Earth', '.']\n"
          ]
        }
      ],
      "source": [
        "data = \"Mars is approximately half the diameter of Earth.\"\n",
        "print(word_tokenize(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fbd03fa",
      "metadata": {
        "id": "9fbd03fa"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e14f298a",
      "metadata": {
        "scrolled": true,
        "id": "e14f298a",
        "outputId": "25904eab-0b37-4ada-f8aa-e703a97b892a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Mars is a cold desert world.', 'It is half the size of Earth.']\n"
          ]
        }
      ],
      "source": [
        "data = \"Mars is a cold desert world. It is half the size of Earth. \"\n",
        "print(sent_tokenize(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b610976",
      "metadata": {
        "id": "9b610976"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import gensim\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3565f53d",
      "metadata": {
        "id": "3565f53d",
        "outputId": "d1fb2b80-0c73-4dd3-e13c-8a83555ded0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of documents: 3\n"
          ]
        }
      ],
      "source": [
        "file_docs = []\n",
        "\n",
        "with open ('data/demofile.txt') as f:\n",
        "    tokens = sent_tokenize(f.read())\n",
        "    for line in tokens:\n",
        "        file_docs.append(line)\n",
        "\n",
        "print(\"Number of documents:\",len(file_docs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbff6563",
      "metadata": {
        "id": "cbff6563",
        "outputId": "eda96d74-1ca8-40e7-a593-2d8c835e989b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['mars',\n",
              "  'is',\n",
              "  'the',\n",
              "  'fourth',\n",
              "  'planet',\n",
              "  'in',\n",
              "  'our',\n",
              "  'solar',\n",
              "  'system',\n",
              "  '.'],\n",
              " ['it',\n",
              "  'is',\n",
              "  'second-smallest',\n",
              "  'planet',\n",
              "  'in',\n",
              "  'the',\n",
              "  'solar',\n",
              "  'system',\n",
              "  'after',\n",
              "  'mercury',\n",
              "  '.'],\n",
              " ['saturn', 'is', 'yellow', 'planet', '.']]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
        "            for text in file_docs]\n",
        "gen_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3d71462",
      "metadata": {
        "id": "f3d71462",
        "outputId": "f09695da-e552-406b-b403-6b8cb8713385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'.': 0, 'fourth': 1, 'in': 2, 'is': 3, 'mars': 4, 'our': 5, 'planet': 6, 'solar': 7, 'system': 8, 'the': 9, 'after': 10, 'it': 11, 'mercury': 12, 'second-smallest': 13, 'saturn': 14, 'yellow': 15}\n"
          ]
        }
      ],
      "source": [
        "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
        "print(dictionary.token2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b97c231",
      "metadata": {
        "id": "3b97c231",
        "outputId": "ae5a17cd-438b-4c45-debf-34663ba24455"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[(0, 1),\n",
              "  (1, 1),\n",
              "  (2, 1),\n",
              "  (3, 1),\n",
              "  (4, 1),\n",
              "  (5, 1),\n",
              "  (6, 1),\n",
              "  (7, 1),\n",
              "  (8, 1),\n",
              "  (9, 1)],\n",
              " [(0, 1),\n",
              "  (2, 1),\n",
              "  (3, 1),\n",
              "  (6, 1),\n",
              "  (7, 1),\n",
              "  (8, 1),\n",
              "  (9, 1),\n",
              "  (10, 1),\n",
              "  (11, 1),\n",
              "  (12, 1),\n",
              "  (13, 1)],\n",
              " [(0, 1), (3, 1), (6, 1), (14, 1), (15, 1)]]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5db1c2c9",
      "metadata": {
        "id": "5db1c2c9",
        "outputId": "d70c7893-ecb0-44ca-9925-a76ddefdee60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['fourth', 0.53], ['in', 0.2], ['mars', 0.53], ['our', 0.53], ['solar', 0.2], ['system', 0.2], ['the', 0.2]]\n",
            "[['in', 0.17], ['solar', 0.17], ['system', 0.17], ['the', 0.17], ['after', 0.47], ['it', 0.47], ['mercury', 0.47], ['second-smallest', 0.47]]\n",
            "[['saturn', 0.71], ['yellow', 0.71]]\n"
          ]
        }
      ],
      "source": [
        "tf_idf = gensim.models.TfidfModel(corpus)\n",
        "for doc in tf_idf[corpus]:\n",
        "    print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1894ca53",
      "metadata": {
        "id": "1894ca53",
        "outputId": "e5783953-b6e4-4e3a-dfbf-c9d951ad9dc1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<gensim.similarities.docsim.Similarity at 0x150a229c280>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sims = gensim.similarities.Similarity('workdir/',tf_idf[corpus],\n",
        "                                        num_features=len(dictionary))\n",
        "sims"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e50ced5a",
      "metadata": {
        "id": "e50ced5a",
        "outputId": "8eb0beee-3514-4b75-cf07-106fffda9e87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of documents: 1\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[(0, 1), (3, 1), (6, 1), (9, 2), (14, 1)]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file2_docs = []\n",
        "\n",
        "with open ('data/demofile2.txt') as f:\n",
        "    tokens = sent_tokenize(f.read())\n",
        "    for line in tokens:\n",
        "        file2_docs.append(line)\n",
        "\n",
        "print(\"Number of documents:\",len(file2_docs))  \n",
        "for line in file2_docs:\n",
        "    query_doc = [w.lower() for w in word_tokenize(line)]\n",
        "    query_doc_bow = dictionary.doc2bow(query_doc) #update an existing dictionary and create bag of words\n",
        "query_doc_bow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebb22333",
      "metadata": {
        "id": "ebb22333",
        "outputId": "503043c8-dfaf-4258-c88d-8990ffb47b76"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(9, 0.5938758662252933), (14, 0.8045566825992793)]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# perform a similarity query against the corpus\n",
        "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
        "query_doc_tf_idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d562011",
      "metadata": {
        "id": "1d562011",
        "outputId": "1d067d38-dd68-4c99-ca21-523f5e47613a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Result: [0.11641413 0.10281226 0.56890744]\n"
          ]
        }
      ],
      "source": [
        "print('Comparing Result:', sims[query_doc_tf_idf]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ee97782",
      "metadata": {
        "id": "6ee97782",
        "outputId": "dd1b165d-6657-4b33-ee59-9d74990e50d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.78813386\n"
          ]
        }
      ],
      "source": [
        "sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
        "print(sum_of_sims)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d07bea6",
      "metadata": {
        "id": "5d07bea6",
        "outputId": "cd619122-ecc7-4ed7-b934-1fee0128e1be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average similarity float: 0.2627112865447998\n",
            "Average similarity percentage: 26.27112865447998\n",
            "Average similarity rounded percentage: 26\n"
          ]
        }
      ],
      "source": [
        "percentage_of_similarity = round(float((sum_of_sims / len(file_docs)) * 100))\n",
        "print(f'Average similarity float: {float(sum_of_sims / len(file_docs))}')\n",
        "print(f'Average similarity percentage: {float(sum_of_sims / len(file_docs)) * 100}')\n",
        "print(f'Average similarity rounded percentage: {percentage_of_similarity}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "name": "similar-sentence.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}